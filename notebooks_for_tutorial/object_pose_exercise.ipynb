{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>A tutorial on 6D object pose estimation for AR application </center>\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"> Ikbeom Jeon (TA) (ikbeomjeon@kaist.ac.k) </div> \n",
    "<div style=\"text-align: right\"> Vincent Lepetit (vincent.lepetit@enpc.fr) </div>  \n",
    "<br>\n",
    "<div style=\"text-align: right\"> Last update: 16/07/2021</div>\n",
    "    \n",
    "\n",
    "**This document is read-only.**  **Please duplicate it to the current folder for you to edit,**  \n",
    "(File-> Save as -> Type your unique name and save.)  \n",
    "And please don't access documents created by others to prevent from losing the edits. \n",
    "\n",
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "In this tutorial, you can try to run the implemented code for state-of-the-art 6D object pose estimation method.\n",
    "\n",
    "It consists of three steps.\n",
    "\n",
    "First, predicting 2D bounding boxs and the initial 6D poses using \"PoseCNN\".  \n",
    "Second, refinining initial object poses using \"DeepIM\" based network that proposed by \"CosyPose\".  \n",
    "Third, apply the results to AR.  \n",
    "\n",
    "There are severel exercises at each step. To solve these, you should look at the reference materials as well as example code in this document.\n",
    "\n",
    "If you have any problems, please contact the TA. \n",
    "\n",
    "\n",
    "### Citation\n",
    "We have referenced the original code needed for writing this tutorial here.\n",
    "https://github.com/ylabbe/cosypose\n",
    "\n",
    "If you use the code in your research, please cite the paper:\n",
    "\n",
    "```\n",
    "@inproceedings{labbe2020,\n",
    "title= {CosyPose: Consistent multi-view multi-object 6D pose estimation}\n",
    "author={Y. {Labbe} and J. {Carpentier} and M. {Aubry} and J. {Sivic}},\n",
    "booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n",
    "year={2020}}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this document.\n",
    "\n",
    "This document is written using \"Jupyter Notbook\". In this document, you can run written code or create new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"If you run this code, click this block(called 'cell') and then press the \\\"Shift + Enter\\\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"If '*' is displayed on left side like 'In[*]:', you should wait for it to complete.\") \n",
    "print(\"Or, click the stop button on menu.\")\n",
    "import time\n",
    "time.sleep(10) # waiting for 10 sec.\n",
    "print(\"Done.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"If you want to save changes, press the \\\"Ctrl+ S\\\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You should run all cells in order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load required packages\n",
    "It is required packages for exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% import package\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## It is required packages for exercises.\n",
    "## you sho\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle as pkl \n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.multiprocessing\n",
    "\n",
    "from bokeh.io import output_notebook, show; output_notebook()\n",
    "from bokeh.plotting import gridplot\n",
    "\n",
    "from cosypose.lib3d import Transform\n",
    "from cosypose.config import EXP_DIR, MEMORY, RESULTS_DIR, LOCAL_DATA_DIR\n",
    "from cosypose.datasets.datasets_cfg import make_scene_dataset, make_object_dataset\n",
    "from cosypose.rendering.bullet_scene_renderer import BulletSceneRenderer\n",
    "from cosypose.visualization.plotter import Plotter\n",
    "from cosypose.visualization.singleview import make_singleview_prediction_plots, filter_predictions\n",
    "from cosypose.visualization.singleview import filter_predictions\n",
    "from cosypose.lib3d.rigid_mesh_database import MeshDataBase\n",
    "from cosypose.rendering.bullet_batch_renderer import BulletBatchRenderer\n",
    "from cosypose.training.pose_models_cfg import create_model_refiner, create_model_coarse, check_update_config\n",
    "from cosypose.models.efficientnet import EfficientNet\n",
    "from cosypose.models.wide_resnet import WideResNet18, WideResNet34\n",
    "from cosypose.models.flownet import flownet_pretrained\n",
    "\n",
    "from cosypose.scripts.run_cosypose_eval import load_posecnn_results\n",
    "from cosypose.visualization.multiview import render_predictions_wrt_camera\n",
    "from cosypose.scripts.run_cosypose_eval import load_posecnn_results\n",
    "\n",
    "# Pose models \n",
    "from cosypose.models.pose import PosePredictor\n",
    "import yaml\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "import cosypose.utils.tensor_collection as tc\n",
    "import random\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(random.randrange(0,4))\n",
    "print(f\"CUDA_VISIBLE_DEVICES : {os.environ['CUDA_VISIBLE_DEVICES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Load and visualize dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_name, urdf_dataset_name = 'ycbv.test.keyframes', 'ycbv'\n",
    "\n",
    "## Load dataset\n",
    "scene_dataset = make_scene_dataset(dataset_name) # Load all frames.\n",
    "\n",
    "## Get sample from dataset\n",
    "scene_id = 51\n",
    "idx = 50\n",
    "\n",
    "mask = scene_dataset.frame_index['scene_id'] == scene_id\n",
    "scene_dataset.frame_index = scene_dataset.frame_index[mask].reset_index(drop=True)\n",
    "\n",
    "input_rgb, _, state = scene_dataset[idx] # Get first sample.\n",
    "\n",
    "view_id = state['frame_info']['view_id']\n",
    "objects = state['objects']  # groundtruths of objects.\n",
    "cameras = [state['camera']] # groundtruths of cameras.\n",
    "\n",
    "print('The first object name and pose :')\n",
    "print(objects[0]['name'])\n",
    "print(objects[0]['TWO'])\n",
    "#print(objects[0][0]['TWO'])\n",
    "\n",
    "print('The extrinsic parameter (i.e. 6D pose) of the camera. :')\n",
    "print(cameras[0]['TWC'])\n",
    "\n",
    "\n",
    "\n",
    "renderer = BulletSceneRenderer(urdf_dataset_name) # Create renderer.\n",
    "objects_rgb = renderer.render_scene(objects, cameras)[0]['rgb'] # Render the scene using object and camera poses.\n",
    "renderer.disconnect() #disconnect renderer.\n",
    "\n",
    "\n",
    "## Plotting images.\n",
    "plotter = Plotter()\n",
    "fig_input_rgb = plotter.plot_image(input_rgb)\n",
    "fig_objects_rgb = plotter.plot_image(objects_rgb)\n",
    "fig_overlay = plotter.plot_overlay(input_rgb, objects_rgb)\n",
    "\n",
    "show(gridplot([[fig_input_rgb, fig_objects_rgb, fig_overlay]], sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "Plot 10th sample of 'scene_id : 51'. \n",
    "\n",
    "You can see other available dataset here.  \n",
    "http://143.248.249.6:9000/tree/local_data/bop_datasets/ycbv/test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "Print the names of symetric obejcts in the scene.\n",
    "\n",
    "You can see the properties of object here.  \n",
    "http://143.248.249.6:9000/tree/cosypose/datasets/bop_object_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "Render the scene after translating the camera by (10, -20, 40) and rotating (30, 10, 40) as euler angles.\n",
    "\n",
    "You can see how to tranform the camera pose here.  \n",
    "4-8 sildes in https://vincentlepetit.github.io/files/pdfs/kaist3_lepetit.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Load results of 2D detection and initial 6D pose estimation from PoseCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "detections = load_posecnn_results()\n",
    "\n",
    "# We can consider object cadidates using its score.\n",
    "mask = (detections.infos['score'] >= 0.0)\n",
    "detections = detections[np.where(mask)[0]]\n",
    "\n",
    "# Find the result corresponding to the input image.\n",
    "det_index = detections.infos['scene_id'] == scene_id\n",
    "detections = detections[np.where(det_index)]\n",
    "det_index = detections.infos['view_id'] == view_id # Use 'view_id' instead of 'idx' to find the sample. \n",
    "detections = detections[np.where(det_index)]\n",
    "\n",
    "print(detections.poses[0])\n",
    "print(detections.bboxes[0])\n",
    "\n",
    "colors = ['yellow' for _ in range(len(detections.poses))]\n",
    "detections.infos['color'] = colors\n",
    "\n",
    "# rendering the 3D scene using result of posecnn.\n",
    "renderer = BulletSceneRenderer(urdf_dataset_name) \n",
    "rendered_pose_coarse = render_predictions_wrt_camera(renderer, detections, state['camera'])\n",
    "renderer.disconnect()\n",
    "\n",
    "fig_detections = fig_input_rgb\n",
    "fig_detections_with_input_rgb = plotter.plot_maskrcnn_bboxes(fig_detections, detections) #Draw 2D bbox in input image\n",
    "fig_rendered_pose_coarse = plotter.plot_overlay(input_rgb, rendered_pose_coarse)\n",
    "\n",
    "show(gridplot([[fig_detections_with_input_rgb, fig_rendered_pose_coarse]], sizing_mode='scale_width'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "What are the input and output of PoseCNN?\n",
    "\n",
    "You can see the parameters in here  \n",
    "line: 70- in http://143.248.249.6:9000/edit/cosypose/scripts/run_cosypose_eval.py\n",
    "\n",
    "If you want to know it more detail, this post will help you.  \n",
    "(Korean) https://juseong-tech.tistory.com/7  \n",
    "(paper) https://arxiv.org/pdf/1711.00199.pdf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "Evaluate the result using ADD and ADD-S metric, and describe why the ADD-S metric was devised.\n",
    "\n",
    "<img src = \"./imgs/add.png\" width=\"60%\">\n",
    "<img src = \"./imgs/add_s.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "You can see the implementation here.  \n",
    "http://143.248.249.6:9000/edit/cosypose/evaluation/meters/pose_meters.py\n",
    "\n",
    "\n",
    "You can see how to use it here.  \n",
    "line: 203-208 in http://143.248.249.6:9000/edit/cosypose/scripts/run_cosypose_eval.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "Find failure case by changing samples and state your opinion in which cases it was faild.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Get better result using refinement network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_id = 'ycbv-n_views=1--5154971130'\n",
    "pred_key = 'posecnn_init/refiner/iteration=2'\n",
    "\n",
    "results = LOCAL_DATA_DIR / 'results' / result_id / 'results.pth.tar'\n",
    "results = torch.load(results)['predictions']\n",
    "\n",
    "results[pred_key].infos.loc[:, ['scene_id', 'view_id']].groupby('scene_id').first()\n",
    "\n",
    "this_preds = filter_predictions(results[pred_key], scene_id, view_id)\n",
    "renderer = BulletSceneRenderer(urdf_dataset_name)\n",
    "figures = make_singleview_prediction_plots(scene_dataset, renderer, this_preds)\n",
    "renderer.disconnect()\n",
    "\n",
    "show(gridplot([[fig_input_rgb, fig_rendered_pose_coarse, figures['pred_overlay'] ]], sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "What are the input and output of renfinement network?\n",
    "\n",
    "You can see the implementation here.  \n",
    "line : 76- in http://143.248.249.6:9000/edit/cosypose/integrated/pose_predictor.py\n",
    "\n",
    "\n",
    "You can see how to use it here.  \n",
    "line: 131-135 in http://143.248.249.6:9000/edit/cosypose/evaluation/pred_runner/multiview_predictions.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "Why did this network use renderer? How is the loss function defined?\n",
    "\n",
    "You can see the basic theory of refinement network here.  \n",
    "slide : 25 in https://vincentlepetit.github.io/files/pdfs/kaist3_lepetit.pdf\n",
    "\n",
    "If you can see more details, please refer to paper.  \n",
    "https://arxiv.org/pdf/1804.00175.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "What is the 'backbone' of this network?  what is the role of that?\n",
    "\n",
    "You can see information of the models here.  \n",
    "http://143.248.249.6:9000/tree/local_data/experiments\n",
    "\n",
    "And you can see how to initialize the network with it.  \n",
    "http://143.248.249.6:9000/tree/cosypose/training/pose_models_cfg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Application for AR\n",
    "\n",
    "In this step, we assumed that images are input in real time, and the processing time of estimation and rendering are fast enough. \n",
    "\n",
    "In order to play video in this document, it is required to record all result images in advance. So it takes quite a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "imgs = []\n",
    "\n",
    "renderer = BulletSceneRenderer('ycbv')  # Create renderer.\n",
    "\n",
    "for sid, sample in enumerate(scene_dataset):\n",
    "    \n",
    "    ## skip some frames due to precessing time :)\n",
    "    if sid % 5 != 0: \n",
    "        continue; \n",
    "        \n",
    "        \n",
    "    input_img, _, state = sample\n",
    "\n",
    "    camera = state['camera']\n",
    "    cameras = [camera]\n",
    "    objects = state['objects']\n",
    "\n",
    "    target_object = dict(name=objects[0]['name'],\n",
    "                         color='white',\n",
    "                         TWO=objects[0]['TWO'])\n",
    "\n",
    "    list_objects = [target_object]\n",
    "\n",
    "    # target_objects_pose = target_objects['TWO']\n",
    "    # fig_overlay = plotter.plot_overlay(input_img, rendered_img)\n",
    "\n",
    "    rendered_img = renderer.render_scene(list_objects, cameras)[0]['rgb']\n",
    "\n",
    "    pil_input_img = Image.fromarray(input_img.numpy())\n",
    "    pil_rendered_img = Image.fromarray(rendered_img)\n",
    "\n",
    "    pil_blend_img = Image.blend(pil_input_img, pil_rendered_img, 0.5)\n",
    "    result_img = np.array(pil_blend_img)\n",
    "\n",
    "    ax_img = ax.imshow(result_img)\n",
    "\n",
    "    imgs.append([ax_img])\n",
    "    \n",
    "\n",
    "renderer.disconnect()\n",
    "ani = animation.ArtistAnimation(fig, imgs, interval=33, blit=True,repeat_delay=1000)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You should call this for playing recored images.\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Execise 4.1 \n",
    "Fill the area of 'cup' with 'yellow' for highlighting that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Execise 4.2 \n",
    "If you want augment your own 3D model or text, please refer these.  \n",
    "\n",
    "http://143.248.249.6:9000/edit/cosypose/recording/bop_recording_scene.py\n",
    "\n",
    "(package they used )https://pybullet.org/wordpress/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3\n",
    "What are the problems of the 6D pose estimation method for augmented reality?\n",
    "\n",
    "Keyword : processing time, occlusion, depth aware, accuracy, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
